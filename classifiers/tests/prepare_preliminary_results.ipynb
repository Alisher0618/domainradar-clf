{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6da3c57-6b29-4cbb-acb8-511c54f94612",
   "metadata": {},
   "source": [
    "# Generating preliminary results for the final aggregation classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7a8113-ad05-423b-9ec2-9f9c5d370cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory changed to: /home/ihranicky/git/domainradar-clf/classifiers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to change to the parent directory\n",
    "def change_to_parent_directory():\n",
    "    # Check if the directory has already been changed\n",
    "    if not os.environ.get('DIR_CHANGED'):\n",
    "        try:\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            current_dir = os.getcwd()\n",
    "        parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "        os.chdir(parent_dir)\n",
    "        os.environ['DIR_CHANGED'] = '1'\n",
    "        print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"Directory has already been changed.\")\n",
    "\n",
    "# Call the function to change the working directory\n",
    "change_to_parent_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153698e5-fbe9-4203-b6e2-468c9367dd26",
   "metadata": {},
   "source": [
    "## Optional: Create testing dataset\n",
    "Note: If you want do to this, set create_test_parquet to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2736c9-87aa-4055-9ff4-f9e05228ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col] = None\n",
      "/tmp/ipykernel_2227180/2797404708.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([combined_df, df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "create_test_parquet = True\n",
    "\n",
    "if create_test_parquet:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # List of input Parquet files along with their maximum rows and desired labels\n",
    "    input_files = [\n",
    "        {'file': 'testdata/phishing_final_2024.parquet', 'label': 'phishing'},\n",
    "        {'file': 'testdata/benign_2312.parquet', 'label': 'benign'},\n",
    "        {'file': 'testdata/umbrella_benign_FINISHED.parquet', 'label': 'benign'},\n",
    "        {'file': 'testdata/malware_bp.parquet', 'label': 'malware'},\n",
    "        {'file': 'testdata/lex-dga-830k-pick.parquet', 'label': 'dga'},\n",
    "    ]\n",
    "    \n",
    "    # Number of rows to select in total\n",
    "    #n_rows = 1000\n",
    "    \n",
    "    # Read the first file to get the initial columns and create the first dataframe\n",
    "    first_file_info = input_files[0]\n",
    "    combined_df = pd.read_parquet(first_file_info['file'])\n",
    "\n",
    "    \n",
    "    # Overwrite the \"label\" column with the specified label for the first file\n",
    "    combined_df['label'] = first_file_info['label']\n",
    "    \n",
    "    # Get the columns from the first dataframe\n",
    "    all_columns = combined_df.columns.tolist()\n",
    "\n",
    "    # Process the remaining files\n",
    "    for file_info in input_files[1:]:\n",
    "        df = pd.read_parquet(file_info['file'])\n",
    "        \n",
    "        # Overwrite the \"label\" column with the specified label\n",
    "        df['label'] = file_info['label']\n",
    "        \n",
    "        # Ensure all columns from the first dataframe are present\n",
    "        for col in all_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Align the dataframe to the columns of the first dataframe\n",
    "        df = df[all_columns]\n",
    "        \n",
    "        # Append the dataframe to the combined dataframe\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Shuffling the DataFrame\n",
    "    shuffled_df = combined_df.sample(frac=1)\n",
    "    \n",
    "    # Save the selected rows to a new Parquet file\n",
    "    shuffled_df.to_parquet('testdata/sample.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279e87c-6092-45a7-9b0b-436ed3299948",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e032f178-d788-4bee-909e-d0c7ce26d3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6cdc-3f2d-4f13-85ea-a6d6ae50d359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffbfb7-dd7b-4066-b0d7-51249fc2d67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc3050e9-4d92-46f5-973b-dc1e69acb882",
   "metadata": {},
   "source": [
    "## Optional: Generate preliminary results for training the final aggregation classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b067ba-4a34-4b6c-ac6d-4a7988a377bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 17:51:57.706907: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-26 17:51:57.706930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-26 17:51:57.707709: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-26 17:51:57.712167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 17:51:58.380982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-26 17:51:58.813985: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-05-26 17:51:58.814005: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:129] retrieving CUDA diagnostic information for host: pchranicky\n",
      "2024-05-26 17:51:58.814009: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:136] hostname: pchranicky\n",
      "2024-05-26 17:51:58.814054: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:159] libcuda reported version is: 535.54.3\n",
      "2024-05-26 17:51:58.814071: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:163] kernel reported version is: 535.54.3\n",
      "2024-05-26 17:51:58.814075: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:241] kernel version seems to match DSO: 535.54.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model created\n",
      "CNN model created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m clf \u001b[38;5;241m=\u001b[39m Pipeline()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Read the input parquet file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m input_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[43mtest_dataset\u001b[49m)\n\u001b[1;32m     11\u001b[0m preliminary_results_df \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mgenerate_preliminary_results(input_df, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreliminary_results.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pipeline import Pipeline\n",
    "\n",
    "# Initialize the classification pipeline\n",
    "clf = Pipeline()\n",
    "\n",
    "# Read the input parquet file\n",
    "input_df = pd.read_parquet(test_dataset)\n",
    "\n",
    "preliminary_results_df = clf.generate_preliminary_results(input_df, output_file=\"preliminary_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b3c08-e107-410c-a167-ea82f1142ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "preliminary_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0dc6-d3ab-4b4f-8ac3-b7faa9d6423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preliminary_results_df[[\"domain_name\", \"label\", \"phishing_cnn_result\", \"phishing_lgbm_result\", \"malware_cnn_result\", \"malware_xgboost_result\", \"dga_binary_nn_result\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb7175-5c65-497b-8d52-e454ba0a65e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94b5da-79ff-47a2-99e5-65ffd80a7e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e196f-6434-426d-b445-c6d61fcf89ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8256beb-f8a0-46f0-acc0-703f1c66e9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
