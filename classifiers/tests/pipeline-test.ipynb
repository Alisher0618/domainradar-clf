{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6da3c57-6b29-4cbb-acb8-511c54f94612",
   "metadata": {},
   "source": [
    "# Test of the clasification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7a8113-ad05-423b-9ec2-9f9c5d370cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory changed to: /home/ihranicky/git/domainradar-clf/classifiers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to change to the parent directory\n",
    "def change_to_parent_directory():\n",
    "    # Check if the directory has already been changed\n",
    "    if not os.environ.get('DIR_CHANGED'):\n",
    "        try:\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            current_dir = os.getcwd()\n",
    "        parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "        os.chdir(parent_dir)\n",
    "        os.environ['DIR_CHANGED'] = '1'\n",
    "        print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"Directory has already been changed.\")\n",
    "\n",
    "# Call the function to change the working directory\n",
    "change_to_parent_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153698e5-fbe9-4203-b6e2-468c9367dd26",
   "metadata": {},
   "source": [
    "## Optional: Create testing dataset\n",
    "Note: If you want do to this, set create_test_parquet to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2736c9-87aa-4055-9ff4-f9e05228ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_parquet = False\n",
    "\n",
    "if create_test_parquet:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # List of input Parquet files\n",
    "    input_files = [\n",
    "        'testdata/misp_2402.parquet',\n",
    "        'testdata/benign_2312.parquet'\n",
    "    ]\n",
    "    \n",
    "    # Number of rows to select\n",
    "    n_rows = 1000\n",
    "    \n",
    "    # Read all Parquet files into a single DataFrame\n",
    "    dataframes = [pd.read_parquet(file) for file in input_files]\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Randomly select n_rows rows from the combined DataFrame\n",
    "    selected_rows = combined_df.sample(n=n_rows, random_state=1)  # random_state for reproducibility\n",
    "    \n",
    "    # Save the selected rows to a new Parquet file\n",
    "    selected_rows.to_parquet('testdata/sample.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279e87c-6092-45a7-9b0b-436ed3299948",
   "metadata": {},
   "source": [
    "## Run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e032f178-d788-4bee-909e-d0c7ce26d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parquet file with the dataset for classification\n",
    "test_dataset = 'testdata/sample.parquet'\n",
    "\n",
    "# Number of domain names to classify with each run of the pipeline (0 = classify all)\n",
    "CHUNK_SIZE = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869c6cdc-3f2d-4f13-85ea-a6d6ae50d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model created\n",
      "===== Processing chunk 1/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "===== Chunk 1/34 completed. =====\n",
      "===== Processing chunk 2/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "===== Chunk 2/34 completed. =====\n",
      "===== Processing chunk 3/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 3/34 completed. =====\n",
      "===== Processing chunk 4/34 =====\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "===== Chunk 4/34 completed. =====\n",
      "===== Processing chunk 5/34 =====\n",
      "[0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 5/34 completed. =====\n",
      "===== Processing chunk 6/34 =====\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 6/34 completed. =====\n",
      "===== Processing chunk 7/34 =====\n",
      "[0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0]\n",
      "===== Chunk 7/34 completed. =====\n",
      "===== Processing chunk 8/34 =====\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "===== Chunk 8/34 completed. =====\n",
      "===== Processing chunk 9/34 =====\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "===== Chunk 9/34 completed. =====\n",
      "===== Processing chunk 10/34 =====\n",
      "[0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 10/34 completed. =====\n",
      "===== Processing chunk 11/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 11/34 completed. =====\n",
      "===== Processing chunk 12/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0]\n",
      "===== Chunk 12/34 completed. =====\n",
      "===== Processing chunk 13/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "===== Chunk 13/34 completed. =====\n",
      "===== Processing chunk 14/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 14/34 completed. =====\n",
      "===== Processing chunk 15/34 =====\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      "===== Chunk 15/34 completed. =====\n",
      "===== Processing chunk 16/34 =====\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "===== Chunk 16/34 completed. =====\n",
      "===== Processing chunk 17/34 =====\n",
      "[0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1]\n",
      "===== Chunk 17/34 completed. =====\n",
      "===== Processing chunk 18/34 =====\n",
      "[0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "===== Chunk 18/34 completed. =====\n",
      "===== Processing chunk 19/34 =====\n",
      "[0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 19/34 completed. =====\n",
      "===== Processing chunk 20/34 =====\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 20/34 completed. =====\n",
      "===== Processing chunk 21/34 =====\n",
      "[0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1]\n",
      "===== Chunk 21/34 completed. =====\n",
      "===== Processing chunk 22/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
      "===== Chunk 22/34 completed. =====\n",
      "===== Processing chunk 23/34 =====\n",
      "[0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1]\n",
      "===== Chunk 23/34 completed. =====\n",
      "===== Processing chunk 24/34 =====\n",
      "[0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 24/34 completed. =====\n",
      "===== Processing chunk 25/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "===== Chunk 25/34 completed. =====\n",
      "===== Processing chunk 26/34 =====\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "===== Chunk 26/34 completed. =====\n",
      "===== Processing chunk 27/34 =====\n",
      "[0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "===== Chunk 27/34 completed. =====\n",
      "===== Processing chunk 28/34 =====\n",
      "[1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "===== Chunk 28/34 completed. =====\n",
      "===== Processing chunk 29/34 =====\n",
      "[1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "===== Chunk 29/34 completed. =====\n",
      "===== Processing chunk 30/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      "===== Chunk 30/34 completed. =====\n",
      "===== Processing chunk 31/34 =====\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "===== Chunk 31/34 completed. =====\n",
      "===== Processing chunk 32/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "===== Chunk 32/34 completed. =====\n",
      "===== Processing chunk 33/34 =====\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1]\n",
      "===== Chunk 33/34 completed. =====\n",
      "===== Processing chunk 34/34 =====\n",
      "[0 0 0 0 1 0 0 1 0 0]\n",
      "===== Chunk 34/34 completed. =====\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pipeline import Pipeline\n",
    "\n",
    "# Initialize the classification pipeline\n",
    "clf = Pipeline()\n",
    "\n",
    "# Read the input parquet file\n",
    "input_df = pd.read_parquet(test_dataset)\n",
    "\n",
    "# Determine the number of chunks\n",
    "num_chunks = (len(input_df) + CHUNK_SIZE - 1) // CHUNK_SIZE if CHUNK_SIZE > 0 else 1\n",
    "\n",
    "# Process the dataframe in chunks\n",
    "for i in range(num_chunks):\n",
    "    if CHUNK_SIZE > 0:\n",
    "        start_idx = i * CHUNK_SIZE\n",
    "        end_idx = start_idx + CHUNK_SIZE\n",
    "        chunk_df = input_df[start_idx:end_idx]\n",
    "    else:\n",
    "        chunk_df = input_df\n",
    "\n",
    "    # Perform your classification or processing on the working_df here\n",
    "    print(f\"===== Processing chunk {i+1}/{num_chunks} =====\")\n",
    "\n",
    "    chunk_without_label = chunk_df.drop(columns=['label']) # Label should not be known to classifiers\n",
    "    chunk_results = clf.classifyDomains(chunk_without_label)\n",
    "\n",
    "     \n",
    "    print(f\"===== Chunk {i+1}/{num_chunks} completed. =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffbfb7-dd7b-4066-b0d7-51249fc2d67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4af69-bbd8-4cfd-89b9-5165b1a78d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
