{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6da3c57-6b29-4cbb-acb8-511c54f94612",
   "metadata": {},
   "source": [
    "# Test of the clasification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7a8113-ad05-423b-9ec2-9f9c5d370cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory changed to: /home/ihranicky/git/domainradar-clf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to change to the parent directory\n",
    "def change_to_parent_directory():\n",
    "    # Check if the directory has already been changed\n",
    "    if not os.environ.get('DIR_CHANGED'):\n",
    "        try:\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            current_dir = os.getcwd()\n",
    "        parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "        os.chdir(parent_dir)\n",
    "        os.environ['DIR_CHANGED'] = '1'\n",
    "        print(f\"Current working directory changed to: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"Directory has already been changed.\")\n",
    "\n",
    "# Call the function to change the working directory\n",
    "change_to_parent_directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153698e5-fbe9-4203-b6e2-468c9367dd26",
   "metadata": {},
   "source": [
    "## Optional: Create testing dataset\n",
    "Note: If you want do to this, set create_test_parquet to **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2736c9-87aa-4055-9ff4-f9e05228ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_test_parquet = False\n",
    "\n",
    "if create_test_parquet:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # List of input Parquet files along with their maximum rows and desired labels\n",
    "    input_files = [\n",
    "        {'file': 'testdata/2405_clftest_benign_filtered.parquet', 'max_rows': 2000, 'label': 'benign'},\n",
    "        {'file': 'testdata/2405_clftest_phishing_filtered.parquet', 'max_rows': 480, 'label': 'phishing'},\n",
    "        #{'file': 'testdata/2405_clftest_malware_filtered.parquet', 'max_rows': 292, 'label': 'malware'},\n",
    "        #{'file': 'testdata/dga_2310.parquet', 'max_rows': 300, 'label': 'dga'},\n",
    "    ]\n",
    "\n",
    "    # Number of rows to select in total\n",
    "    n_rows = 2480\n",
    "    \n",
    "    # Read the first file to get the initial columns and create the first dataframe\n",
    "    first_file_info = input_files[0]\n",
    "    combined_df = pd.read_parquet(first_file_info['file'])\n",
    "    \n",
    "    # Limit the number of rows if necessary for the first file\n",
    "    if len(combined_df) > first_file_info['max_rows']:\n",
    "        combined_df = combined_df.sample(n=first_file_info['max_rows'], random_state=1)\n",
    "    \n",
    "    # Overwrite the \"label\" column with the specified label for the first file\n",
    "    combined_df['label'] = first_file_info['label']\n",
    "    \n",
    "    # Get the columns from the first dataframe\n",
    "    all_columns = combined_df.columns.tolist()\n",
    "\n",
    "    # Process the remaining files\n",
    "    for file_info in input_files[1:]:\n",
    "        df = pd.read_parquet(file_info['file'])\n",
    "        \n",
    "        # Limit the number of rows if necessary\n",
    "        if len(df) > file_info['max_rows']:\n",
    "            df = df.sample(n=file_info['max_rows'], random_state=1)\n",
    "        \n",
    "        # Overwrite the \"label\" column with the specified label\n",
    "        df['label'] = file_info['label']\n",
    "        \n",
    "        # Ensure all columns from the first dataframe are present\n",
    "        for col in all_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "        \n",
    "        # Align the dataframe to the columns of the first dataframe\n",
    "        df = df[all_columns]\n",
    "        \n",
    "        # Append the dataframe to the combined dataframe\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    \n",
    "    # Randomly select n_rows rows from the combined DataFrame\n",
    "    selected_rows = combined_df.sample(n=n_rows, random_state=1)  # random_state for reproducibility\n",
    "    \n",
    "    # Save the selected rows to a new Parquet file\n",
    "    selected_rows.to_parquet('testdata/debug.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d294c35-9459-4400-8e59-1d1037743ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /home/ihranicky/git/domainradar-clf/.venv/lib/python3.11/site-packages/classifiers-0.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/ihranicky/git/domainradar-clf/.venv/lib/python3.11/site-packages/classifiers-0.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Skipping /home/ihranicky/git/domainradar-clf/.venv/lib/python3.11/site-packages/classifiers-0.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279e87c-6092-45a7-9b0b-436ed3299948",
   "metadata": {},
   "source": [
    "## Run classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e032f178-d788-4bee-909e-d0c7ce26d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parquet file with the dataset for classification\n",
    "test_dataset = 'testdata/debug.parquet'\n",
    "\n",
    "# Number of domain names to classify with each run of the pipeline (0 = classify all)\n",
    "CHUNK_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869c6cdc-3f2d-4f13-85ea-a6d6ae50d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ihranicky/git/domainradar-clf/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-02 16:40:38.339900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Processing chunk 1/50 =====\n",
      "Res | Domain Name                                        | Actual Label       | Predicted  | Probability | Phi LGBM  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'LightGBM phishing classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m domain_name \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     80\u001b[0m aggregate_probability \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregate_probability\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 81\u001b[0m phi_lbgm \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLightGBM phishing classifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     82\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_labels[idx] \u001b[38;5;241m==\u001b[39m true_labels[idx] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mER\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_format_str\u001b[38;5;241m.\u001b[39mformat(status, domain_name[:\u001b[38;5;241m50\u001b[39m], actual_label, predicted_label, aggregate_probability, phi_lbgm))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LightGBM phishing classifier'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from classifiers.pipeline import Pipeline\n",
    "from classifiers.options import PipelineOptions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the classification pipeline\n",
    "clf_options = PipelineOptions()\n",
    "clf = Pipeline(clf_options)\n",
    "\n",
    "# Read the input parquet file\n",
    "input_df = pd.read_parquet(test_dataset)\n",
    "\n",
    "# Function to map labels to 'benign' or 'malign'\n",
    "def map_label(label):\n",
    "    if label == 'benign':\n",
    "        return 'benign'\n",
    "    else:\n",
    "        return f'malign ({label})'\n",
    "\n",
    "# Function to convert labels to binary classes\n",
    "def binary_label(label):\n",
    "    return 'negative' if label == 'benign' else 'positive'\n",
    "\n",
    "# Apply label mapping\n",
    "input_df['mapped_label'] = input_df['label'].apply(map_label)\n",
    "input_df['binary_label'] = input_df['label'].apply(binary_label)\n",
    "\n",
    "# Determine the number of chunks\n",
    "num_chunks = (len(input_df) + CHUNK_SIZE - 1) // CHUNK_SIZE if CHUNK_SIZE > 0 else 1\n",
    "\n",
    "# Initialize counters for overall statistics\n",
    "total_true_labels = []\n",
    "total_pred_labels = []\n",
    "\n",
    "# Initialize dictionaries to keep track of feature scores\n",
    "feature_scores_correct = {}\n",
    "feature_scores_incorrect = {}\n",
    "\n",
    "# Format string for aligned output\n",
    "header_format_str = \"{:<3} | {:<50} | {:<18} | {:<10} | {:<10} | {:<10}\"\n",
    "data_format_str = \"{:<3} | {:<50} | {:<18} | {:<10} | {:.6f} | {:<10}\"\n",
    "\n",
    "# Process the dataframe in chunks\n",
    "for i in range(num_chunks):\n",
    "    if CHUNK_SIZE > 0:\n",
    "        start_idx = i * CHUNK_SIZE\n",
    "        end_idx = start_idx + CHUNK_SIZE\n",
    "        chunk_df = input_df[start_idx:end_idx]\n",
    "    else:\n",
    "        chunk_df = input_df\n",
    "\n",
    "    # Perform your classification or processing on the working_df here\n",
    "    print(f\"===== Processing chunk {i+1}/{num_chunks} =====\")\n",
    "\n",
    "    chunk_without_label = chunk_df.drop(columns=['label', 'mapped_label', 'binary_label']) # Label should not be known to classifiers\n",
    "    chunk_results = clf.classify_domains(chunk_without_label)\n",
    "\n",
    "    # Collect predictions and true labels\n",
    "    true_labels = chunk_df['binary_label'].values\n",
    "    pred_labels = []\n",
    "    for result in chunk_results:\n",
    "        pred_label = 'negative' if result['aggregate_probability'] < 0.5 else 'positive'\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    # Update overall statistics\n",
    "    total_true_labels.extend(true_labels)\n",
    "    total_pred_labels.extend(pred_labels)\n",
    "\n",
    "    # Display header for results\n",
    "    print(header_format_str.format(\"Res\", \"Domain Name\", \"Actual Label\", \"Predicted\", \"Probability\", \"Phi LGBM\"))\n",
    "\n",
    "    # Display results for each domain\n",
    "    for idx, result in enumerate(chunk_results):\n",
    "        actual_label = chunk_df.iloc[idx]['mapped_label']\n",
    "        predicted_label = 'benign' if pred_labels[idx] == 'negative' else 'malign'\n",
    "        domain_name = result['domain_name']\n",
    "        aggregate_probability = result['aggregate_probability']\n",
    "        phi_lbgm = result['classification_results'][0]['details']['LightGBM phishing classifier']\n",
    "        status = \"OK\" if pred_labels[idx] == true_labels[idx] else \"ER\"\n",
    "        print(data_format_str.format(status, domain_name[:50], actual_label, predicted_label, aggregate_probability, phi_lbgm))\n",
    "        \n",
    "        # Run debug_domain method for domains\n",
    "        debug_data = clf.debug_domain(domain_name, chunk_df, n_top_features=10)\n",
    "        \n",
    "        for classifier, data in debug_data.items():\n",
    "            for feature_info in data['top_features']:\n",
    "                feature = feature_info['feature']\n",
    "                shap_value = feature_info['shap_value']\n",
    "                \n",
    "                # Determine if the feature's SHAP value is GOOD or BAD\n",
    "                is_positive_label = true_labels[idx] == 'positive'\n",
    "                feature_status = \"GOOD\" if (is_positive_label and shap_value >= 0) or (not is_positive_label and shap_value < 0) else \"BAAD\"\n",
    "                \n",
    "                # Update feature scores\n",
    "                if feature_status == \"GOOD\":\n",
    "                    if feature in feature_scores_correct:\n",
    "                        feature_scores_correct[feature] += shap_value\n",
    "                    else:\n",
    "                        feature_scores_correct[feature] = shap_value\n",
    "                else:\n",
    "                    if feature in feature_scores_incorrect:\n",
    "                        feature_scores_incorrect[feature] += shap_value\n",
    "                    else:\n",
    "                        feature_scores_incorrect[feature] = shap_value\n",
    "\n",
    "                print(f\"- {feature_status} Feature: {feature}, Value: {feature_info['value']}, SHAP Value: {shap_value}\")\n",
    "\n",
    "    # Calculate metrics for the current chunk\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, pos_label='positive', average='binary')\n",
    "    recall = recall_score(true_labels, pred_labels, pos_label='positive', average='binary')\n",
    "    f1 = f1_score(true_labels, pred_labels, pos_label='positive', average='binary')\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels, labels=['negative', 'positive']).ravel()\n",
    "    false_positives = fp\n",
    "    false_negatives = fn\n",
    "    total_positives = tp + fp\n",
    "    total_negatives = tn + fn\n",
    "\n",
    "    fp_ratio = (false_positives / total_positives) if total_positives > 0 else 0\n",
    "    fn_ratio = (false_negatives / total_negatives) if total_negatives > 0 else 0\n",
    "\n",
    "    print(f\"Chunk {i+1}/{num_chunks} metrics:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"False Positives: {false_positives} ({fp_ratio * 100:.2f}%)\")\n",
    "    print(f\"False Negatives: {false_negatives} ({fn_ratio * 100:.2f}%)\")\n",
    "    print(f\"===== Chunk {i+1}/{num_chunks} completed. =====\")\n",
    "\n",
    "    # Calculate and print the merged feature scores for the current chunk\n",
    "    merged_feature_scores = {}\n",
    "    for feature in set(feature_scores_correct.keys()).union(feature_scores_incorrect.keys()):\n",
    "        correct_score = feature_scores_correct.get(feature, 0)\n",
    "        incorrect_score = feature_scores_incorrect.get(feature, 0)\n",
    "        merged_feature_scores[feature] = incorrect_score - correct_score  # or incorrect_score / correct_score if you prefer\n",
    "\n",
    "    # Print top features for correct and incorrect classifications for the current chunk\n",
    "    print(\"\\nTop features responsible for correct classifications in this chunk:\")\n",
    "    for feature, score in sorted(feature_scores_correct.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
    "        print(f\"Feature: {feature}, Score: {score}\")\n",
    "\n",
    "    print(\"\\nTop features responsible for incorrect classifications in this chunk:\")\n",
    "    for feature, score in sorted(feature_scores_incorrect.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
    "        print(f\"Feature: {feature}, Score: {score}\")\n",
    "\n",
    "    print(\"\\nMerged feature scores (incorrect - correct) in this chunk:\")\n",
    "    for feature, score in sorted(merged_feature_scores.items(), key=lambda item: item[1], reverse=True)[:10]:\n",
    "        print(f\"Feature: {feature}, Score: {score}\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_accuracy = accuracy_score(total_true_labels, total_pred_labels)\n",
    "overall_precision = precision_score(total_true_labels, total_pred_labels, pos_label='positive', average='binary')\n",
    "overall_recall = recall_score(total_true_labels, total_pred_labels, pos_label='positive', average='binary')\n",
    "overall_f1 = f1_score(total_true_labels, total_pred_labels, pos_label='positive', average='binary')\n",
    "\n",
    "overall_tn, overall_fp, overall_fn, overall_tp = confusion_matrix(total_true_labels, total_pred_labels, labels=['negative', 'positive']).ravel()\n",
    "overall_false_positives = overall_fp\n",
    "overall_false_negatives = overall_fn\n",
    "overall_total_positives = overall_tp + overall_fp\n",
    "overall_total_negatives = overall_tn + overall_fn\n",
    "\n",
    "overall_fp_ratio = (overall_false_positives / overall_total_positives) if overall_total_positives > 0 else 0\n",
    "overall_fn_ratio = (overall_false_negatives / overall_total_negatives) if overall_total_negatives > 0 else 0\n",
    "\n",
    "print(\"Overall metrics:\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
    "print(f\"Overall Precision: {overall_precision}\")\n",
    "print(f\"Overall Recall: {overall_recall}\")\n",
    "print(f\"Overall F1 Score: {overall_f1}\")\n",
    "print(f\"Overall False Positives: {overall_false_positives} ({overall_fp_ratio * 100:.2f}%)\")\n",
    "print(f\"Overall False Negatives: {overall_false_negatives} ({overall_fn_ratio * 100:.2f}%)\")\n",
    "\n",
    "# Calculate and print the merged feature scores overall\n",
    "merged_feature_scores_overall = {}\n",
    "for feature in set(feature_scores_correct.keys()).union(feature_scores_incorrect.keys()):\n",
    "    correct_score = feature_scores_correct.get(feature, 0)\n",
    "    incorrect_score = feature_scores_incorrect.get(feature, 0)\n",
    "    merged_feature_scores_overall[feature] = incorrect_score - correct_score  # or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0c487-161d-41de-b694-ee484ce50040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e3f23-603c-447e-bc76-a3277898a1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92068cd-4213-439e-94ce-1acedf5ed5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
